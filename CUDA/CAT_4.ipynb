{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bR5q4jgwd_Fb"
   },
   "source": [
    "# CAT: 4 - CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fwZbrke-GAeP"
   },
   "source": [
    "## 1 CUDA Execution Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G6ELcCB3EAvT",
    "outputId": "682e8159-0c6d-4adb-b0f2-325e1bc08c01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
      "Cuda compilation tools, release 11.8, V11.8.89\n",
      "Build cuda_11.8.r11.8/compiler.31833905_0\n"
     ]
    }
   ],
   "source": [
    "! nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0fP5risDEWog",
    "outputId": "3f690af3-e963-4454-fbbe-fdde99043f8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
      "  Cloning https://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-tx1m4v7m\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-tx1m4v7m\n",
      "  Resolved https://github.com/andreinechaev/nvcc4jupyter.git to commit aac710a35f52bb78ab34d2e52517237941399eff\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Building wheels for collected packages: NVCCPlugin\n",
      "  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-py3-none-any.whl size=4287 sha256=321c937882579f848dcb06c51984e84778d1374898ca5878cebf5e4d471a50c1\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-k7yvhvrt/wheels/a8/b9/18/23f8ef71ceb0f63297dd1903aedd067e6243a68ea756d6feea\n",
      "Successfully built NVCCPlugin\n",
      "Installing collected packages: NVCCPlugin\n",
      "Successfully installed NVCCPlugin-0.0.2\n",
      "created output directory at /content/src\n",
      "Out bin /content/result.out\n"
     ]
    }
   ],
   "source": [
    "! pip install \\\n",
    "git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
    "%load_ext nvcc_plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jLyxPVS8FwTb",
    "outputId": "3518e3ee-c235-4c3b-881d-2095eff6be58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world from the device!\n",
      "Hello world from the device!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%cu\n",
    "\n",
    "#include <stdio.h>\n",
    "\n",
    "__global__ void hello_kernel(void) {\n",
    "    printf(\"Hello world from the device!\\n\");\n",
    "}\n",
    "\n",
    "int main(void) {\n",
    "    printf(\"Hello world from the device!\\n\");\n",
    "    hello_kernel<<<1,1>>>();\n",
    "    cudaDeviceSynchronize();\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZhtnKkSGRAh"
   },
   "source": [
    "## 2 Threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T0pfndOnK9-_",
    "outputId": "7318c941-0db6-447e-b6d8-5418dabc6706"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi! My Id is 8, I am the thread 0 out of 4 in block 2\n",
      "Hi! My Id is 9, I am the thread 1 out of 4 in block 2\n",
      "Hi! My Id is 10, I am the thread 2 out of 4 in block 2\n",
      "Hi! My Id is 11, I am the thread 3 out of 4 in block 2\n",
      "Hi! My Id is 20, I am the thread 0 out of 4 in block 5\n",
      "Hi! My Id is 21, I am the thread 1 out of 4 in block 5\n",
      "Hi! My Id is 22, I am the thread 2 out of 4 in block 5\n",
      "Hi! My Id is 23, I am the thread 3 out of 4 in block 5\n",
      "Hi! My Id is 0, I am the thread 0 out of 4 in block 0\n",
      "Hi! My Id is 1, I am the thread 1 out of 4 in block 0\n",
      "Hi! My Id is 2, I am the thread 2 out of 4 in block 0\n",
      "Hi! My Id is 3, I am the thread 3 out of 4 in block 0\n",
      "Hi! My Id is 16, I am the thread 0 out of 4 in block 4\n",
      "Hi! My Id is 17, I am the thread 1 out of 4 in block 4\n",
      "Hi! My Id is 18, I am the thread 2 out of 4 in block 4\n",
      "Hi! My Id is 19, I am the thread 3 out of 4 in block 4\n",
      "Hi! My Id is 12, I am the thread 0 out of 4 in block 3\n",
      "Hi! My Id is 13, I am the thread 1 out of 4 in block 3\n",
      "Hi! My Id is 14, I am the thread 2 out of 4 in block 3\n",
      "Hi! My Id is 15, I am the thread 3 out of 4 in block 3\n",
      "Hi! My Id is 4, I am the thread 0 out of 4 in block 1\n",
      "Hi! My Id is 5, I am the thread 1 out of 4 in block 1\n",
      "Hi! My Id is 6, I am the thread 2 out of 4 in block 1\n",
      "Hi! My Id is 7, I am the thread 3 out of 4 in block 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%cu\n",
    "\n",
    "#include <stdio.h>\n",
    "\n",
    "__global__ void hello_kernel(void) {\n",
    "    int blockId = blockIdx.x + blockIdx.y * gridDim.x + blockIdx.z * gridDim.x * gridDim.y;\n",
    "    int threadId = blockId * blockDim.x + threadIdx.x;\n",
    "    printf(\"Hi! My Id is %d, I am the thread %d out of %d in block %d\\n\", threadId, threadIdx.x, blockDim.x, blockId);\n",
    "}\n",
    "\n",
    "int main(void) {\n",
    "    int gridDimension = 6;\n",
    "    int blockDimension = 4;\n",
    "    hello_kernel<<<gridDimension, blockDimension>>>();\n",
    "    cudaDeviceSynchronize();\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ugKPGXRRv7b"
   },
   "source": [
    "## 3 Memory Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "psp1azDNZ08x"
   },
   "source": [
    "* Would it work the code without using the function “cudaMemcpy”.\n",
    "\n",
    "Without cudaMemcpy calls, the device would not have access to the initial data in array a, and the modified data would not be transferred back to the host. As a result, the final print statement would not display the modified array a correctly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z2GnpVK0aS-_"
   },
   "source": [
    "* Change the value of “BLOCKSIZE” to, for instance, “4”.\n",
    "How does it affect the execution compared to the original output?\n",
    "\n",
    "Original output was \"Hello World!\". After setting `BLOCKSIZE = 4` result is \"Hello Worlo\". The program will modify characters in `a` by adding the matching integer values from array `b`. By this process, each adding will generate the given result, for instance `a[0] + b[0]` will be 'H' + 15 = 72, 72 is the chacter 'W' in ASCCI, 'e' + 10 = 111 wich is 'o' and so on. However, the program will only be capable to read `BLOCKSIZE` elements of each and since `BLOCKSIZE` is set to 4, the threads within a block will have indices from 0 to 3, hence, the last characters from `b` will not be read and when added to `a` it will give us the remaining of `Hello` string, in this case `o`. `BLOCKSIZE` must have a minimum value of 6 in order to perform correclty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HM4lBOtqVyha",
    "outputId": "94464525-b8ab-4050-d857-d579a11969d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Worlo \n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%cu\n",
    "#include <stdio.h>\n",
    "\n",
    "const int N = 16;\n",
    "const int GRIDSIZE = 1;   //number of thread blocks\n",
    "const int BLOCKSIZE = 4; //number of threads per thread block\n",
    "\n",
    "__global__ void hello_decoder(char *a, int *b) {\n",
    "    a[threadIdx.x] += b[threadIdx.x];\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    char a[N] = \"Hello \\0\\0\\0\\0\\0\\0\";\n",
    "    int b[N] = {15, 10, 6, 0, -11, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0};\n",
    "    char *ad;\n",
    "    int *bd;\n",
    "    const int csize = N*sizeof(char);\n",
    "    const int isize = N*sizeof(int);\n",
    "\n",
    "    printf(\"%s\", a);\n",
    "\n",
    "    cudaMalloc( (void**)&ad, csize );\n",
    "    cudaMalloc( (void**)&bd, isize );\n",
    "    cudaMemcpy( ad, a, csize, cudaMemcpyHostToDevice );\n",
    "    cudaMemcpy( bd, b, isize, cudaMemcpyHostToDevice );\n",
    "\n",
    "    hello_decoder<<<GRIDSIZE, BLOCKSIZE>>>(ad, bd);\n",
    "    cudaMemcpy( a, ad, csize, cudaMemcpyDeviceToHost );\n",
    "    cudaFree( ad );\n",
    "    cudaFree( bd );\n",
    "\n",
    "    printf(\"%s\\n\", a);\n",
    "    return EXIT_SUCCESS;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Om2qNPLd1oj"
   },
   "source": [
    "## 4 Accelerator Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eougkpn-gGaa",
    "outputId": "0f0505ac-9b19-494f-d65c-7197f1378bb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May 28 21:46:16 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   39C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4-JNO_8FWsGS"
   },
   "source": [
    "|   Model  |  Architecture  | SMs | Cores |\n",
    "|:--------:|:--------------:|:---:|:-----:|\n",
    "| Tesla T4 | Turing (TU104) |  40 |  2560 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ejTmhaHgUjC"
   },
   "source": [
    "## 5 Programming Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B5raz3SigZ-D",
    "outputId": "ea882380-c1a0-4aab-dada-7395b2c8731e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing exercise.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile exercise.cu\n",
    "\n",
    "#include <iostream>\n",
    "#include <math.h>\n",
    "#define VALUE 20\n",
    "#define PROBLEMSIZE 1000000000\n",
    "\n",
    "__global__ void add(float *x, float *y, int size) {\n",
    "    int i = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "    if (i < size)\n",
    "      y[i] = x[i] + y[i];\n",
    "}\n",
    "\n",
    "int main(void) {\n",
    "  float *x, *y;\n",
    "  cudaMallocManaged(&x, PROBLEMSIZE*sizeof(float));\n",
    "  cudaMallocManaged(&y, PROBLEMSIZE*sizeof(float));\n",
    "  for (int i = 0; i < PROBLEMSIZE; i++) {\n",
    "    float val = (float)(i % VALUE);\n",
    "    x[i] = val;\n",
    "    y[i] = (VALUE - val);\n",
    "  }\n",
    "  int blockSize = 256;\n",
    "  int numBlocks = (PROBLEMSIZE + blockSize - 1) / blockSize;\n",
    "  add<<<numBlocks, blockSize>>>(x, y, PROBLEMSIZE);\n",
    "  cudaDeviceSynchronize();\n",
    "  float error = 0.0f;\n",
    "  for (int i = 0; i < PROBLEMSIZE; i++)\n",
    "    error = fmax(error, fabs(y[i]-VALUE));\n",
    "  if (error != 0)\n",
    "    printf(\"Wrong result. Check your code, especially your kernel\\n\");\n",
    "  cudaFree(x);\n",
    "  cudaFree(y);\n",
    "  return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JeN28Sn4im-F"
   },
   "outputs": [],
   "source": [
    "! nvcc -o exercise exercise.cu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NBkb4dTYl0RH",
    "outputId": "71779a1e-ef43-4598-c413-c6e65df3cf4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 122 ms, sys: 11.7 ms, total: 134 ms\n",
      "Wall time: 18.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "! ./exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2kuarJg1jFVB"
   },
   "source": [
    "* How different is managed transfers between CPU and GPU?\n",
    "\n",
    "Managed transfers is handled automatically by the CUDA runtime system. This approach simplifies memory management by allowing the programmer to allocate and access memory using a unified memory address space. Unlike explicit memory transfers, where it has to be done manually data gest copied between CPU and GPU memory, managed transfers automatically migrate data between CPU and GPU as needed. It also introduces some performance overhead due to the additional management and data migration operations. However, managed transfers provide convenience and flexibility by abstracting away low-level memory management tasks and enabling more efficient utilization of system resources in many scenarios and better programming experience.\n",
    "\n",
    "* Check that it does not return an error (you can attach a screenshot).\n",
    "\n",
    "* How long does it take to run (you can use the extension %%time at the beginning of the cell, or the Unix command time before the binary execution)?\n",
    "\n",
    "![screenshot.png](data:image/png;base64,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)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0-LQBr59k4FI"
   },
   "source": [
    "* Provide block/thread configurations that reduces the execution time. Compare the configurations and the execution time (for instance, with a table)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "eD0j4tilqxZA",
    "outputId": "d62c1c97-df29-4779-9cb4-d68515b08d06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting configurations.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile configurations.cu\n",
    "\n",
    "#include <iostream>\n",
    "#include <math.h>\n",
    "#include <stdio.h>\n",
    "#define VALUE 20\n",
    "#define PROBLEMSIZE 1000000000\n",
    "\n",
    "__global__ void add(float *x, float *y, int size) {\n",
    "    int i = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "    if (i < size)\n",
    "      y[i] = x[i] + y[i];\n",
    "}\n",
    "\n",
    "int main(void) {\n",
    "  float *x, *y;\n",
    "  cudaMallocManaged(&x, PROBLEMSIZE * sizeof(float));\n",
    "  cudaMallocManaged(&y, PROBLEMSIZE * sizeof(float));\n",
    "  for (int i = 0; i < PROBLEMSIZE; i++) {\n",
    "    float val = (float)(i % VALUE);\n",
    "    x[i] = val;\n",
    "    y[i] = (VALUE - val);\n",
    "  }\n",
    "\n",
    "  int configurations[][2] = {\n",
    "    {256, 256},\n",
    "    {512, 128},\n",
    "    {1024, 64},\n",
    "  };\n",
    "  \n",
    "  printf(\"Configuration\\t|  Execution Time (ms)\\n\");\n",
    "  printf(\"-------------------------------------\\n\");\n",
    "\n",
    "  // Perform computations for each configuration and measure the execution time\n",
    "  for (int config = 0, numConfigurations = sizeof(configurations) / sizeof(configurations[0]); config < numConfigurations; config++) {\n",
    "    int blockSize = configurations[config][0];\n",
    "    int numBlocks = (PROBLEMSIZE + blockSize - 1) / blockSize;\n",
    "\n",
    "    // Start measuring execution time\n",
    "    cudaEvent_t start, end;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&end);\n",
    "    cudaEventRecord(start);\n",
    "\n",
    "    add<<<numBlocks, blockSize>>>(x, y, PROBLEMSIZE);\n",
    "    cudaDeviceSynchronize();\n",
    "\n",
    "    // Stop measuring execution time\n",
    "    cudaEventRecord(end);\n",
    "    cudaEventSynchronize(end);\n",
    "    float milliseconds = 0;\n",
    "    cudaEventElapsedTime(&milliseconds, start, end);\n",
    "\n",
    "    // Print the configuration and its corresponding execution time\n",
    "    printf(\"%d threads, %d blocks  | %.2f\\n\", blockSize, numBlocks, milliseconds);\n",
    "  }\n",
    "\n",
    "  cudaFree(x);\n",
    "  cudaFree(y);\n",
    "  return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "1E9EQwKXq5E7",
    "outputId": "2a6980b7-51d5-4da3-97f0-6170799f9dc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration\t|  Execution Time (ms)\n",
      "-------------------------------------\n",
      "256 threads, 3906250 blocks  | 2034.24\n",
      "512 threads, 1953125 blocks  | 46.67\n",
      "1024 threads, 976563 blocks  | 46.72\n"
     ]
    }
   ],
   "source": [
    "! nvcc -o configurations configurations.cu && ./configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DI7rc_SMxjIm"
   },
   "source": [
    "* Is there any limitation on the blocks or threads quantities?\n",
    "\n",
    "You can have up to 2^31 - 1 blocks in the x-dimension, and at most 65535 blocks in the y and z dimensions and a maximum of 1024 threads per block.\n",
    "\n",
    "\\* [CUDA C++ Programming Guide - 6.2. Features and Technical Specifications](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#features-and-technical-specifications)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bNgpvpQEjm6l"
   },
   "source": [
    "## 6 Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5DzrnCTkjrnf",
    "outputId": "505ec9ad-7b6d-44e6-ddf3-25cc8e8cc911"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==28754== NVPROF is profiling process 28754, command: ./exercise\n",
      "==28754== Profiling application: ./exercise\n",
      "==28754== Profiling result:\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      " GPU activities:  100.00%  2.77920s         1  2.77920s  2.77920s  2.77920s  add(float*, float*, int)\n",
      "      API calls:   77.21%  2.77921s         1  2.77921s  2.77921s  2.77921s  cudaDeviceSynchronize\n",
      "                   15.92%  572.99ms         2  286.50ms  236.12ms  336.87ms  cudaFree\n",
      "                    6.84%  246.17ms         2  123.09ms  51.124us  246.12ms  cudaMallocManaged\n",
      "                    0.02%  889.17us         1  889.17us  889.17us  889.17us  cuDeviceGetPCIBusId\n",
      "                    0.00%  113.31us       101  1.1210us     128ns  47.690us  cuDeviceGetAttribute\n",
      "                    0.00%  63.917us         1  63.917us  63.917us  63.917us  cudaLaunchKernel\n",
      "                    0.00%  23.675us         1  23.675us  23.675us  23.675us  cuDeviceGetName\n",
      "                    0.00%  1.5400us         3     513ns     160ns  1.1470us  cuDeviceGetCount\n",
      "                    0.00%     870ns         2     435ns     156ns     714ns  cuDeviceGet\n",
      "                    0.00%     535ns         1     535ns     535ns     535ns  cuModuleGetLoadingMode\n",
      "                    0.00%     492ns         1     492ns     492ns     492ns  cuDeviceTotalMem\n",
      "                    0.00%     257ns         1     257ns     257ns     257ns  cuDeviceGetUuid\n",
      "\n",
      "==28754== Unified Memory profiling result:\n",
      "Device \"Tesla T4 (0)\"\n",
      "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
      "  111732  67.225KB  4.0000KB  0.9961MB  7.163231GB  880.2939ms  Host To Device\n",
      "   22894  170.62KB  4.0000KB  0.9961MB  3.725292GB  341.6113ms  Device To Host\n",
      "   10846         -         -         -           -   2.758979s  Gpu page fault groups\n",
      "Total CPU Page faults: 34341\n"
     ]
    }
   ],
   "source": [
    "! nvprof ./exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UeJFyEmljwLk"
   },
   "source": [
    "Provide the code of the non-managed CPU-GPU memory\n",
    "version of the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VK_SIPJlj0nF",
    "outputId": "29ea1bf9-55b3-4214-8c07-28677218f238"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting non_managed.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile non_managed.cu\n",
    "\n",
    "#include <iostream>\n",
    "#include <math.h>\n",
    "#define VALUE 20\n",
    "#define PROBLEMSIZE 1000\n",
    "\n",
    "__global__ void add(float *x, float *y, int size) {\n",
    "    int i = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "    if (i < size)\n",
    "        y[i] = x[i] + y[i];\n",
    "}\n",
    "\n",
    "int main(void) {\n",
    "    float *x, *y;\n",
    "    float *d_x, *d_y;\n",
    "    int size = PROBLEMSIZE * sizeof(float);\n",
    "\n",
    "    // Allocate memory on CPU\n",
    "    x = (float*)malloc(size);\n",
    "    y = (float*)malloc(size);\n",
    "\n",
    "    // Initialize input arrays on CPU\n",
    "    for (int i = 0; i < PROBLEMSIZE; i++) {\n",
    "        float val = (float)(i % VALUE);\n",
    "        x[i] = val;\n",
    "        y[i] = (VALUE - val);\n",
    "    }\n",
    "\n",
    "    // Allocate memory on GPU\n",
    "    cudaMalloc(&d_x, size);\n",
    "    cudaMalloc(&d_y, size);\n",
    "\n",
    "    // Copy input arrays from CPU to GPU\n",
    "    cudaMemcpy(d_x, x, size, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_y, y, size, cudaMemcpyHostToDevice);\n",
    "\n",
    "    // Launch kernel\n",
    "    int blockSize = 1;\n",
    "    int numBlocks = (PROBLEMSIZE + blockSize - 1) / blockSize;\n",
    "    add<<<numBlocks, blockSize>>>(d_x, d_y, PROBLEMSIZE);\n",
    "    cudaDeviceSynchronize();\n",
    "\n",
    "    // Copy result array from GPU to CPU\n",
    "    cudaMemcpy(y, d_y, size, cudaMemcpyDeviceToHost);\n",
    "\n",
    "    // Check for errors\n",
    "    float error = 0.0f;\n",
    "    for (int i = 0; i < PROBLEMSIZE; i++)\n",
    "        error = fmax(error, fabs(y[i] - VALUE));\n",
    "    if (error != 0)\n",
    "        printf(\"Wrong result. Check your code, especially your kernel\\n\");\n",
    "\n",
    "    // Free memory on GPU\n",
    "    cudaFree(d_x);\n",
    "    cudaFree(d_y);\n",
    "\n",
    "    // Free memory on CPU\n",
    "    free(x);\n",
    "    free(y);\n",
    "\n",
    "    return 0;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zk4t5H_cnyBe",
    "outputId": "a5530070-0a03-4aa4-bae8-0b94c21b90e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==1322== NVPROF is profiling process 1322, command: ./non_managed\n",
      "==1322== Profiling application: ./non_managed\n",
      "==1322== Profiling result:\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      " GPU activities:   55.64%  7.4240us         1  7.4240us  7.4240us  7.4240us  add(float*, float*, int)\n",
      "                   27.09%  3.6150us         2  1.8070us  1.6960us  1.9190us  [CUDA memcpy HtoD]\n",
      "                   17.27%  2.3040us         1  2.3040us  2.3040us  2.3040us  [CUDA memcpy DtoH]\n",
      "      API calls:   99.66%  340.34ms         2  170.17ms  8.2480us  340.33ms  cudaMalloc\n",
      "                    0.24%  804.43us         1  804.43us  804.43us  804.43us  cuDeviceGetPCIBusId\n",
      "                    0.04%  125.67us       101  1.2440us     142ns  52.676us  cuDeviceGetAttribute\n",
      "                    0.03%  102.13us         2  51.063us  6.3910us  95.735us  cudaFree\n",
      "                    0.02%  61.743us         3  20.581us  9.3790us  28.301us  cudaMemcpy\n",
      "                    0.01%  38.888us         1  38.888us  38.888us  38.888us  cudaLaunchKernel\n",
      "                    0.01%  25.743us         1  25.743us  25.743us  25.743us  cuDeviceGetName\n",
      "                    0.00%  9.8090us         1  9.8090us  9.8090us  9.8090us  cudaDeviceSynchronize\n",
      "                    0.00%  2.1110us         3     703ns     218ns  1.6630us  cuDeviceGetCount\n",
      "                    0.00%  1.0790us         2     539ns     192ns     887ns  cuDeviceGet\n",
      "                    0.00%     649ns         1     649ns     649ns     649ns  cuModuleGetLoadingMode\n",
      "                    0.00%     512ns         1     512ns     512ns     512ns  cuDeviceTotalMem\n",
      "                    0.00%     236ns         1     236ns     236ns     236ns  cuDeviceGetUuid\n"
     ]
    }
   ],
   "source": [
    "! nvcc -o non_managed non_managed.cu && nvprof ./non_managed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZMqs1dETj1F7"
   },
   "source": [
    "* Which are the dominant API calls when the block size is equal to \"1\"? Why?\n",
    "\n",
    "`cudaMalloc` with accounts for 99.66% of the API calls. The majority of the time is spent on allocating GPU memory because the block size. With a small block size, more individual memory allocations are required leading to that increased overhead.\n",
    "\n",
    "\n",
    "\n",
    "* Would the dominant API calls be the same in the managed memory version? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GdudE8Fq8aOl",
    "outputId": "bff43122-8c0e-4f0d-b472-7107db1089cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==2678== NVPROF is profiling process 2678, command: ./managed\n",
      "==2678== Profiling application: ./managed\n",
      "==2678== Profiling result:\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      " GPU activities:  100.00%  3.56911s         1  3.56911s  3.56911s  3.56911s  add(float*, float*, int)\n",
      "      API calls:   81.99%  3.56914s         1  3.56914s  3.56914s  3.56914s  cudaDeviceSynchronize\n",
      "                   12.17%  530.02ms         2  265.01ms  235.74ms  294.28ms  cudaFree\n",
      "                    5.83%  254.02ms         2  127.01ms  58.169us  253.96ms  cudaMallocManaged\n",
      "                    0.00%  118.48us       101  1.1730us     131ns  48.039us  cuDeviceGetAttribute\n",
      "                    0.00%  74.836us         1  74.836us  74.836us  74.836us  cudaLaunchKernel\n",
      "                    0.00%  25.015us         1  25.015us  25.015us  25.015us  cuDeviceGetName\n",
      "                    0.00%  7.3720us         1  7.3720us  7.3720us  7.3720us  cuDeviceGetPCIBusId\n",
      "                    0.00%  2.6080us         3     869ns     222ns  2.1200us  cuDeviceGetCount\n",
      "                    0.00%  1.0220us         2     511ns     155ns     867ns  cuDeviceGet\n",
      "                    0.00%     419ns         1     419ns     419ns     419ns  cuDeviceTotalMem\n",
      "                    0.00%     408ns         1     408ns     408ns     408ns  cuModuleGetLoadingMode\n",
      "                    0.00%     276ns         1     276ns     276ns     276ns  cuDeviceGetUuid\n",
      "\n",
      "==2678== Unified Memory profiling result:\n",
      "Device \"Tesla T4 (0)\"\n",
      "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
      "  110850  67.672KB  4.0000KB  0.9961MB  7.154015GB  878.6278ms  Host To Device\n",
      "   22894  170.62KB  4.0000KB  0.9961MB  3.725292GB  343.8263ms  Device To Host\n",
      "   10926         -         -         -           -   3.550738s  Gpu page fault groups\n",
      "Total CPU Page faults: 34341\n"
     ]
    }
   ],
   "source": [
    "! nvcc -o managed exercise.cu && nvprof ./managed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q_pMUfEg8p2l"
   },
   "source": [
    "This time the dominants APIs are:\n",
    "\n",
    "* cudaDeviceSynchronize (81.99%): the majority of the time  of this code is spent waiting for the kernel to finish its execution since it is the primary computation in this code.\n",
    "\n",
    "* cudaFree (12.17%) and cudaMallocManaged (5.83%) because large amount of memory is being allocated."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
